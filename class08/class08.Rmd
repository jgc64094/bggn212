---
title: "Machine Learning 1"
author: "Jose Chacon (PID A59010515)"
date: "10/22/2021"
output: github_document
---


#Clustering methods


kmeans clusterin in R is done with the `kmeans()` function.
Here we makeup some data to test and learn with.

```{r}
tmp <- c(rnorm(30,3), rnorm(30,-3) )
data <- cbind(x=tmp,y=rev(tmp)) #Note- making dataset with reverse order on y axis
plot(data)
#hist(tmp)
```

Run `kmeans()` set Kto 2 nstart 20. The thing with kmeans is that you have to tell it how many clusters you want.

```{r}
km <- kmeans(data, centers=2, nstart=20)
km
```
>Q1. How many points are in each cluster?

```{r}
km$size
```

>Q. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```
>Q. What 'component'of your result object details cluster center?

```{r}
km$centers
```

>Q. Plot x colored by the kmeans cluster assignment and the cluster center as blue points

```{r}
plot(data, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```

#Hierarchical clustering

We will use the 'hclust()' function on the same data as before and see how this method works
```{r}
hc <- hclust( dist(data) )
hc
```

hclust has a plot method
```{r}
plot(hc)
#obline(h=7), col"red") #visualize height cut
```

TO find our membership we need to "cut" the treeand for this we use the `cutree()` function and tellthe height to cut it

```{r}
cutree(hc, h=7)
```

We can alse use `cutree()` and state thte number of k clusters we want..

```{r}
grps <- cutree(hc, k=2)
```


```{r}
plot(data, col=grps)
```

#Notes from board
kmeans(x, centers=?)
hclust(dist(x))

#Principal Componenet Analysis (PCA)

Import data from CSV file
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

How many rows and cols?
```{r}
dim(x)
```
```{r}
x[,-1]
```

```{r}
rownames(x) <- x[,1]
x <- x[-1]
x #distructive way of writng
```

```{r}

url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
dim(x)
```

Ë†
```{r}
barplot( as.matrix(x), col=rainbow(17))
```

```{r}
barplot( as.matrix(x), col=rainbow(17), beside=TRUE)
```
```{r}
mycols <- rainbow( nrow(x))
pairs(x, col=mycols, pch=10)
```

## PCA to the rescue!

Here we will use the base R function for PCA, which is called `pccomp()`. This functions wants the transpose data 

```{r}
pca <- prcomp( t (x) )
summary(pca)
```

```{r}
plot(pca)
```

We want score plot (a.k.a. PCA plot). Basically of PC1 vs PC2

```{r}
attributes(pca)
```

we are after the pca$x compenent 
```{r}
plot(pca$x[,1:2])
text(pca$x[,1:2], labels = colnames(x))
```

We can also examine the PCA "loadings", which tell us how much the original variables contribute to each new PC...

```{r}
#par(mar=c(30,3,0.35,0)) #error
barplot(pca$rotation[,1], las=2)
```


##One more PCA for today

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
nrow(rna.data)
```

```{r}
ncol(rna.data)
```

```{r}
colnames(rna.data)
```

```{r}
pca.rna <- prcomp( t (rna.data), scale= TRUE)
summary(pca.rna)
plot(pca.rna)
```

```{r}
plot(pca.rna$x[,1:2])
text(pca.rna$x[,1:2], labels = colnames(rna.data)) #error
```
